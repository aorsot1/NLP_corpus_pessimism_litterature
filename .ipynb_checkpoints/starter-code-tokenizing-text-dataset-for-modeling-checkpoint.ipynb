{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f3acd60",
   "metadata": {
    "papermill": {
     "duration": 0.015456,
     "end_time": "2021-07-25T23:49:00.375884",
     "exception": false,
     "start_time": "2021-07-25T23:49:00.360428",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# STARTER CODE: Tokenizing Text Dataset for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c229b",
   "metadata": {
    "papermill": {
     "duration": 0.009649,
     "end_time": "2021-07-25T23:49:00.396762",
     "exception": false,
     "start_time": "2021-07-25T23:49:00.387113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4922691",
   "metadata": {
    "papermill": {
     "duration": 0.009425,
     "end_time": "2021-07-25T23:49:00.416009",
     "exception": false,
     "start_time": "2021-07-25T23:49:00.406584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befe9ee",
   "metadata": {
    "papermill": {
     "duration": 0.009494,
     "end_time": "2021-07-25T23:49:00.435326",
     "exception": false,
     "start_time": "2021-07-25T23:49:00.425832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdbe5aa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T23:49:00.461256Z",
     "iopub.status.busy": "2021-07-25T23:49:00.459504Z",
     "iopub.status.idle": "2021-07-25T23:49:06.003306Z",
     "shell.execute_reply": "2021-07-25T23:49:06.003893Z",
     "shell.execute_reply.started": "2021-07-25T23:14:22.874613Z"
    },
    "papermill": {
     "duration": 5.559054,
     "end_time": "2021-07-25T23:49:06.004195",
     "exception": false,
     "start_time": "2021-07-25T23:49:00.445141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d2dff4",
   "metadata": {
    "papermill": {
     "duration": 0.009844,
     "end_time": "2021-07-25T23:49:06.024212",
     "exception": false,
     "start_time": "2021-07-25T23:49:06.014368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Print Directory Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1d0bb71",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-25T23:49:06.047961Z",
     "iopub.status.busy": "2021-07-25T23:49:06.046865Z",
     "iopub.status.idle": "2021-07-25T23:49:06.061719Z",
     "shell.execute_reply": "2021-07-25T23:49:06.061145Z",
     "shell.execute_reply.started": "2021-07-25T23:05:07.415529Z"
    },
    "papermill": {
     "duration": 0.027725,
     "end_time": "2021-07-25T23:49:06.061859",
     "exception": false,
     "start_time": "2021-07-25T23:49:06.034134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/political-though-work-corpus/political_thought_works_corpus.csv\n",
      "/kaggle/input/political-though-work-corpus/all-data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e09a97b",
   "metadata": {
    "papermill": {
     "duration": 0.010716,
     "end_time": "2021-07-25T23:49:06.083182",
     "exception": false,
     "start_time": "2021-07-25T23:49:06.072466",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5758392f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T23:49:06.107164Z",
     "iopub.status.busy": "2021-07-25T23:49:06.106524Z",
     "iopub.status.idle": "2021-07-25T23:49:07.894493Z",
     "shell.execute_reply": "2021-07-25T23:49:07.893438Z",
     "shell.execute_reply.started": "2021-07-25T23:05:08.291193Z"
    },
    "papermill": {
     "duration": 1.800981,
     "end_time": "2021-07-25T23:49:07.894658",
     "exception": false,
     "start_time": "2021-07-25T23:49:06.093677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Medium</th>\n",
       "      <th>Link</th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Philosophy</td>\n",
       "      <td>Book</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/1497</td>\n",
       "      <td>Produced by Sue Asscher      THE REPUBLIC  By ...</td>\n",
       "      <td>Plato</td>\n",
       "      <td>The Republic</td>\n",
       "      <td>No Date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Philosophy</td>\n",
       "      <td>Book</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/1998</td>\n",
       "      <td>Produced by Sue Asscher      THUS SPAKE ZARATH...</td>\n",
       "      <td>Friedrich Nietzsche</td>\n",
       "      <td>Thus Spake Zarathustra</td>\n",
       "      <td>No Date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Philosophy</td>\n",
       "      <td>Book</td>\n",
       "      <td>https://www.gutenberg.org/ebooks/4363</td>\n",
       "      <td>Produced by John Mamoun, Charles Franks and th...</td>\n",
       "      <td>Friedrich Nietzsche</td>\n",
       "      <td>Beyond Good and Evil</td>\n",
       "      <td>No Date</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Subject Medium                                   Link  \\\n",
       "0  Philosophy   Book  https://www.gutenberg.org/ebooks/1497   \n",
       "1  Philosophy   Book  https://www.gutenberg.org/ebooks/1998   \n",
       "2  Philosophy   Book  https://www.gutenberg.org/ebooks/4363   \n",
       "\n",
       "                                                Text               Author  \\\n",
       "0  Produced by Sue Asscher      THE REPUBLIC  By ...                Plato   \n",
       "1  Produced by Sue Asscher      THUS SPAKE ZARATH...  Friedrich Nietzsche   \n",
       "2  Produced by John Mamoun, Charles Franks and th...  Friedrich Nietzsche   \n",
       "\n",
       "                    Title     Date  \n",
       "0            The Republic  No Date  \n",
       "1  Thus Spake Zarathustra  No Date  \n",
       "2    Beyond Good and Evil  No Date  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/kaggle/input/political-though-work-corpus/all-data.csv')\n",
    "data = data[data['Text'].apply(lambda x:isinstance(x, str))==True]\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae1647",
   "metadata": {
    "papermill": {
     "duration": 0.012049,
     "end_time": "2021-07-25T23:49:07.918228",
     "exception": false,
     "start_time": "2021-07-25T23:49:07.906179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac10ab7b",
   "metadata": {
    "papermill": {
     "duration": 0.011732,
     "end_time": "2021-07-25T23:49:07.941391",
     "exception": false,
     "start_time": "2021-07-25T23:49:07.929659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vectorize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31134fba",
   "metadata": {
    "papermill": {
     "duration": 0.011327,
     "end_time": "2021-07-25T23:49:07.964813",
     "exception": false,
     "start_time": "2021-07-25T23:49:07.953486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This script collects a list of texts and converts them to a padded, tokenized TensorFlow dataset. Because almost all the string-level operations are performed within `tf.strings`, the process takes very little time to process large quantities of text (about two-thirds of a minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0be20e03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T23:49:08.054255Z",
     "iopub.status.busy": "2021-07-25T23:49:08.015994Z",
     "iopub.status.idle": "2021-07-25T23:49:45.633522Z",
     "shell.execute_reply": "2021-07-25T23:49:45.634059Z",
     "shell.execute_reply.started": "2021-07-25T23:38:42.078211Z"
    },
    "papermill": {
     "duration": 37.658235,
     "end_time": "2021-07-25T23:49:45.634236",
     "exception": false,
     "start_time": "2021-07-25T23:49:07.976001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 37.622 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "'''\n",
    "====================================================================================\n",
    "START OF RELEVANT TOKENIZATION SCRIPT\n",
    "====================================================================================\n",
    "'''\n",
    "\n",
    "# importing necessary function\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# collect training data\n",
    "train_data = data['Text'].tolist()\n",
    "\n",
    "# quickly count number of unique words\n",
    "complete_text = tf.strings.join([tf.constant(text) for text in data['Text']])\n",
    "y, idx, count = tf.unique_with_counts(tf.strings.split(complete_text))\n",
    "\n",
    "# set important parameters\n",
    "num_words = y.shape[0]\n",
    "oov_token = '<UNK>'\n",
    "pad_type = 'post'\n",
    "trunc_type = 'post'\n",
    "\n",
    "# define and fit tokenizer\n",
    "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "\n",
    "# pad sequences\n",
    "maxlen = max([len(x) for x in train_sequences])\n",
    "train_padded = pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\n",
    "train_padded = tf.constant(train_padded)\n",
    "\n",
    "# create tensorflow dataset\n",
    "data = tf.data.Dataset.from_tensor_slices(train_padded)\n",
    "\n",
    "'''\n",
    "====================================================================================\n",
    "END OF RELEVANT TOKENIZATION SCRIPT\n",
    "====================================================================================\n",
    "'''\n",
    "\n",
    "end = time.time()\n",
    "print(f'Took {round(end-start,3)} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfe7ba0",
   "metadata": {
    "papermill": {
     "duration": 0.010845,
     "end_time": "2021-07-25T23:49:45.656149",
     "exception": false,
     "start_time": "2021-07-25T23:49:45.645304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can 'detokenize' a vectorization by passing it through `tokenizer.sequences_to_texts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e681661",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T23:49:45.684666Z",
     "iopub.status.busy": "2021-07-25T23:49:45.683397Z",
     "iopub.status.idle": "2021-07-25T23:49:47.757377Z",
     "shell.execute_reply": "2021-07-25T23:49:47.756883Z",
     "shell.execute_reply.started": "2021-07-25T23:48:20.937088Z"
    },
    "papermill": {
     "duration": 2.090256,
     "end_time": "2021-07-25T23:49:47.757537",
     "exception": false,
     "start_time": "2021-07-25T23:49:45.667281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'produced by sue asscher the republic by plato translated by benjamin jowett note the republic by plato jowett etext 150 introduction and analysis the republic of plato is the longest of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are more clearly drawn out in the laws as works of art the symposium and the protagoras are of higher excellence but no other dialogue of plato has the same largeness of view and the same perfection of style no other shows an equal knowledge of the world or contains more of those thoughts which are new as well as old and not of one age only but of all nowhere in plato is there a deeper irony or a greater wealth of humour or imagery or more dramatic power nor in any other of his writings is the attempt made to interweave life and speculation or to connect politics '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_string = tokenizer.sequences_to_texts(train_padded.numpy()[0:1])[0]\n",
    "decoded_string[:1000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 57.803026,
   "end_time": "2021-07-25T23:49:50.096738",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-25T23:48:52.293712",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
