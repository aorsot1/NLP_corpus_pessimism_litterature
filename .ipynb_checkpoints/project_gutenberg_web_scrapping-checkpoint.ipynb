{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Gutenberg Web Scrapping\n",
    "In this notebook, we will web scraping from Project Gutenberg & Google to obtain the content and other informations of political philosophy texts. Those would be the building blocks of a database for Natural Language Processing (NLP) purposes open-source for anyone interested in the intersection between data science and political thought. \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "1. Environment set-up\n",
    "    * Importing Libraries\n",
    "2. Books Web Scraping\n",
    "    * Variable Definition\n",
    "    * Info Details Extraction\n",
    "    * Text Content Extraction\n",
    "3. Data Cleaning\n",
    "    * Null Values\n",
    "    * Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "_cell_guid": "758cd899-bfd5-4be6-a65a-666bcf8f7a63",
    "_uuid": "d2302def-fe6a-4a69-9656-1cddb7559487",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "# We scraping tools\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Dataframe manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text manipulation\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Books Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "As a start, we defined a list with authors' names to scrape Project Gutenberg. We obtain that list researching influential thikers in the political philosophy tradition. As one may note, it is heavily biased towards western throught, but as we progress, we will enlarge our perspective with texts from across the world. We eventually hope a comprehensive library for Natural Language Processing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing a few authors:\n",
    "# Hannah Arendt, Étienne de La Boétie, Carl Schmitt,\n",
    "# Simone de Beauvoir, Jean-Paul Sartre, Michel Foucault\n",
    "# Jacques Derrida, Gilles Deleuze, Jean Baudrillard,\n",
    "# W. E. B. Dubois, Aimé Cesaire, Leopold Senghor\n",
    "author_names = [\n",
    "    'Plato','Aristotle','Rousseau, Jean-Jacques', 'Hegel, Georg Wilhelm Friedrich'\n",
    "    'Hume, David', 'Locke, John', 'Machiavelli, Niccolò','Mill, John Stuart',\n",
    "    'Kant, Immanuel','Nietzsche, Friedrich Wilhelm', 'Hobbes, Thomas',\n",
    "    'Montesquieu, Charles de Secondat, baron de','Russell, Bertrand',\n",
    "    'Burke, Edmund', 'Priestley', 'Spencer, Herbert', 'Comte, Auguste',\n",
    "    'Epictetus','Bodin, Jean','Godwin, William','Harrington, James',\n",
    "    'Jellinek, Georg', 'Lieber, Francis', 'Proudhon, P.-J. (Pierre-Joseph)',\n",
    "    'Labriola, Antonio', 'Dante Alighieri', 'Jefferson, Thomas', 'Adams, John',\n",
    "    'Croce, Benedetto', 'Marx, Karl', 'Montaigne, Michel de', \n",
    "    'Sunzi, active 6th century B.C.', 'Anarchism Emma Goldman',\n",
    "    'A Vindication of the Rights of Woman Wollstonecraft, Mary',\n",
    "    'Bakunin, Mikhail Aleksandrovich', 'Kropotkin',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "Our first steps in scraping Project Gutenberg, we will get details on the books such as title, topics/themes, language, etc. It would help contextualize without having the text content as of yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean it up & make it a class object\n",
    "# Handling errors & exceptions\n",
    "def book_details_extraction(names):\n",
    "    # Formatting author names for API request\n",
    "    books_formatted = []\n",
    "    \n",
    "    for name in names:\n",
    "        url = 'http://gutendex.com/books?search='+name.replace(' ', '%20')\n",
    "        \n",
    "        req = requests.get(url)\n",
    "        soup = BeautifulSoup(req.text, 'html.parser')\n",
    "        books = json.loads(soup.text)['results']\n",
    "        \n",
    "        for book in books:\n",
    "            if len(book['authors']) > 0:\n",
    "                book_authors = book['authors'][0]\n",
    "            else:\n",
    "                author_details = ''\n",
    "            text_urls = book['formats']\n",
    "            res = [val for key, val in text_urls.items() if 'text/plain' in key \n",
    "                                                        and '.txt' in val]\n",
    "            topics = book['subjects']+book['bookshelves']\n",
    "            lang = book['languages'][0]\n",
    "            book.update({'name':book_authors})\n",
    "            book.update({'topics':topics})\n",
    "            book.update({'language':lang})\n",
    "            if len(res) == 0:\n",
    "                book.update({'text_url':''})\n",
    "            else:\n",
    "                book.update({'text_url':res[0]})  \n",
    "\n",
    "            keys_to_remove = ['id', 'authors','translators', 'subjects',\n",
    "                          'bookshelves', 'languages','copyright', \n",
    "                          'media_type','formats','download_count']\n",
    "            for key in keys_to_remove:\n",
    "                book.pop(key)\n",
    "            \n",
    "            books_formatted.append(book)\n",
    "    \n",
    "    return pd.json_normalize(books_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_details_extraction(author_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "In this section, we will proceed in extracting the texts under three main formats: its full body, its broken paragraphs, and its sentences. It would give more flexibility in terms of NLP projects and the various forms they make take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting everything into a Class with different methods\n",
    "class GutenbergTextRetrieveal():\n",
    "    ''' \n",
    "    A class extracting the details and content of texts \n",
    "    pulled from Project Gutenberg\n",
    "    '''\n",
    "    def __init__(self, links):\n",
    "        self.links = links\n",
    "    \n",
    "    def text_scraper(self):\n",
    "        self.texts = []\n",
    "\n",
    "        content_begs = ['*** START OF', '***START OF']\n",
    "        content_ends = ['*** END OF ', '*** END OF ',]\n",
    "        for link in self.links:\n",
    "            # Details on the text\n",
    "            req = requests.get(link)\n",
    "            soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "            raw_text = soup.text\n",
    "            \n",
    "            for beg in content_begs:\n",
    "                if raw_text.find(beg) == -1:\n",
    "                    continue\n",
    "                else:\n",
    "                    idx0 = raw_text.find(beg)\n",
    "                    \n",
    "            for end in content_ends:\n",
    "                if raw_text.find(end) == -1:\n",
    "                    continue\n",
    "                else:\n",
    "                    idx1 = raw_text.find(end)\n",
    "                    \n",
    "            text_body = raw_text[idx0:idx1].replace(content_beg, '')\n",
    "            text_body = re.sub(r'http\\S+', '', text_body)\n",
    "            text_body = re.sub('\\W+',' ', text_body)\n",
    "            \n",
    "            self.texts.append(text_body)\n",
    "        return self.texts\n",
    "\n",
    "# Base URL\n",
    "# links = list(hume_texts['text_url'])\n",
    "# ret = GutenbergTextRetrieveal(links)\n",
    "# texts = ret.text_scraper()\n",
    "# hume_texts['text_content'] = texts\n",
    "# hume_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Think about the inclusion of paragraphs & sentences delimitations\n",
    "# req = requests.get('https://www.gutenberg.org/files/4320/4320-h/4320-h.htm')\n",
    "# soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "# paragraph=soup.find_all(\"p\")\n",
    "# for para in paragraph:\n",
    "#     print(para.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Think about duplicates\n",
    "## Consider multiple authors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
